## About

This repo contains examples of **airbyte components** that can be used for data transformation and custom connectors.

## What is ELT?

Extract/load/transform (ELT) is the process of extracting data from one or multiple sources and loading it into a target data warehouse. Instead of transforming the data before itâ€™s written, ELT takes advantage of the target system to do the data transformation

## What is Airbyte?

[Airbyte](https://airbyte.com/) is an open-source data integration platform that syncs data from applications, APIs & databases to data warehouses, lakes, and other destinations.
<p align="center">
  <img src="https://i.ibb.co/cgr5xsD/Screen-Shot-2022-09-27-at-9-50-10-AM.png" alt="Airbyte"/>
</p>


## Usecases

**Data Engineering** - Build custom connectors in hours and get them maintained through Airbyte<br />
**Data Analytics** - Centralize all your data with Airbyte's long tail of pre-built connectors<br />
**Data Science** - Consolidated all your data sources in your data lake for your algorithms<br />
**Engineering** - Leverage Airbyte's API and CLI to build your data systems or products<br />

## Deployment

Airbyte can be deployed with docker

**AWS EC2** - [Setup Airbyte with EC2 instance](https://docs.airbyte.com/deploying-airbyte/on-aws-ec2/)

## Customization

**DBT** - [Leverage *Data Build Tool* to customize data transformations](https://docs.airbyte.com/operator-guides/transformation-and-normalization/transformations-with-dbt/)

**Custom Connector** - [Write your custom connector with Airbyte CDK](https://airbyte.gitbook.io/airbyte/connector-development/cdk-python)

## Further Readings
- [Airbyte](https://airbyte.com/)
- [ELT](https://www.talend.com/resources/what-is-elt/)
- [Datalake](https://aws.amazon.com/big-data/datalakes-and-analytics/what-is-a-data-lake/)
- [Datamart](https://www.oracle.com/ph/autonomous-database/what-is-data-mart)
- [Datawarehouse](https://aws.amazon.com/data-warehouse/)
- [Usecases](https://airbyte.com/tutorials)
- [Best Practices](https://airbyte.com/blog/best-practices-data-ingestion-pipeline)



